# âœ¨ PixieAI

A local AI assistant for macOS powered by MLX and Gemma, with optional internet search.

![Version](https://img.shields.io/badge/version-1.0.0-blue)
![Python](https://img.shields.io/badge/python-3.14+-blue)
![Platform](https://img.shields.io/badge/platform-macOS%20(Apple%20Silicon)-lightgrey)
![License](https://img.shields.io/badge/license-MIT-green)

## Features

- ğŸš€ **Fast Local Inference** - Runs Gemma 2 9B on Apple Silicon using MLX
- ğŸ” **Internet Search** - Optional DuckDuckGo integration for up-to-date answers
- ğŸ¨ **Native macOS UI** - Beautiful chatbot-style interface with message bubbles
- ğŸ’¾ **Memory Efficient** - 4-bit quantization fits in <16GB RAM
- ğŸ”’ **Private** - All processing happens locally on your Mac
- âš¡ **Streaming** - Real-time token-by-token responses
- ğŸ’¬ **Conversation Memory** - Remembers chat context within session

## Screenshots

The app features a clean chatbot interface with:
- Separate message bubbles for user and assistant
- Status indicator (Online, Thinking, Searching)
- Web Search toggle for internet-enhanced answers

## Requirements

- macOS with Apple Silicon (M1/M2/M3)
- 16GB RAM recommended
- Python 3.14+
- [uv](https://github.com/astral-sh/uv) package manager

## Installation

```bash
# Clone the repository
git clone https://github.com/Bikitsos/PixieAI.git
cd PixieAI

# Install dependencies with uv
uv sync
```

## Usage

```bash
# Run the application
uv run main.py
```

On first run, the app will download the Gemma model (~5-6GB). This only happens once.

## Building macOS App

Build a standalone .app bundle:

```bash
# Build the app
uv run pyinstaller PixieAI.spec --noconfirm

# The app will be in dist/PixieAI.app
open dist/PixieAI.app
```

You can copy `PixieAI.app` to your Applications folder.

### Features

- **Chat**: Type your question and press Enter or click Send
- **Web Search**: Toggle to include DuckDuckGo search results in responses
- **Streaming**: Responses appear token-by-token in real-time

## Project Structure

```
PixieAI/
â”œâ”€â”€ main.py              # Entry point
â”œâ”€â”€ assets/
â”‚   â””â”€â”€ icon.png         # App icon
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ app.py           # Application launcher
â”‚   â”œâ”€â”€ config.py        # Configuration settings
â”‚   â”œâ”€â”€ version.py       # Version information
â”‚   â”œâ”€â”€ gui/
â”‚   â”‚   â”œâ”€â”€ main_window.py   # Chatbot UI with message bubbles
â”‚   â”‚   â””â”€â”€ worker.py        # Background thread for LLM
â”‚   â”œâ”€â”€ llm/
â”‚   â”‚   â””â”€â”€ wrapper.py       # MLX LLM wrapper with conversation memory
â”‚   â””â”€â”€ search/
â”‚       â””â”€â”€ __init__.py      # DuckDuckGo search
â”œâ”€â”€ hooks/                   # PyInstaller hooks for MLX
â”œâ”€â”€ pyproject.toml
â””â”€â”€ ROADMAP.md
```

## Configuration

Edit `src/config.py` to customize:

- `MODEL_ID` - Hugging Face model to use
- `MAX_TOKENS` - Maximum response length
- `TEMPERATURE` - Creativity (0.0-1.0)
- `MAX_SEARCH_RESULTS` - Number of web results

## Tech Stack

- **MLX** - Apple's ML framework for Apple Silicon
- **Gemma 2 9B (4-bit)** - Google's efficient language model
- **PyQt6** - Cross-platform GUI framework
- **ddgs** - DuckDuckGo search API

## Changelog

### v1.0.0
- Initial release
- Chatbot-style UI with message bubbles
- Conversation memory within session
- Web search integration
- Streaming responses
- macOS .app bundle support

## License

MIT License - see [LICENSE](LICENSE) for details.
